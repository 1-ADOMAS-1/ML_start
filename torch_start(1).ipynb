{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M1QbinaTdoUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear, ReLU, Sigmoid, Softmax, Sequential, MSELoss, BCELoss, CrossEntropyLoss"
      ],
      "metadata": {
        "id": "RGXkGUOtwD1a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Линейные слои & Ф.Активации"
      ],
      "metadata": {
        "id": "i-JaNwcDdpYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(4,2)\n",
        "W = torch.randn(2,3)\n",
        "layer = Linear(2,3)\n",
        "\n",
        "print(X, W, X @ W, layer.weight, layer(X), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IHAyzjlGPnI",
        "outputId": "87888bd9-9ae1-4341-d95f-c6109bd89d99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1569,  0.4879],\n",
            "        [-0.7986,  0.6605],\n",
            "        [-1.8552,  0.5687],\n",
            "        [ 0.4638,  1.0925]])\n",
            "tensor([[-0.5361, -0.1343,  0.7091],\n",
            "        [-0.4250, -0.9717, -0.5993]])\n",
            "tensor([[-0.1232, -0.4530, -0.4036],\n",
            "        [ 0.1475, -0.5346, -0.9622],\n",
            "        [ 0.7529, -0.3035, -1.6564],\n",
            "        [-0.7129, -1.1239, -0.3258]])\n",
            "Parameter containing:\n",
            "tensor([[-0.5327,  0.0139],\n",
            "        [-0.3103,  0.6112],\n",
            "        [-0.1701,  0.1980]], requires_grad=True)\n",
            "tensor([[-0.3264, -0.0693, -0.0701],\n",
            "        [ 0.0179,  0.2353,  0.0732],\n",
            "        [ 0.5795,  0.5071,  0.2347],\n",
            "        [-0.6486,  0.1076, -0.0560]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(3,3)\n",
        "layer = ReLU()\n",
        "layer(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CnAjKSGHOQm",
        "outputId": "c612b11e-34dc-4b4d-e74b-ce5c48586db9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6663, 0.0000, 0.0000],\n",
              "        [1.7150, 0.0000, 0.0000],\n",
              "        [0.0000, 1.3142, 1.1931]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = torch.randn(3, 1)\n",
        "layer = Sigmoid()\n",
        "layer(X1), X1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91l9v_tjHQi4",
        "outputId": "914d741e-1647-4958-f305-3133b8b32228"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.3507],\n",
              "         [0.4716],\n",
              "         [0.5296]]),\n",
              " tensor([[-0.6159],\n",
              "         [-0.1137],\n",
              "         [ 0.1186]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Softmax()\n",
        "layer(X), X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGVeWS01H6E2",
        "outputId": "99b650b0-ca49-491d-9520-19bd020549da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.6337, 0.0754, 0.2909],\n",
              "         [0.8756, 0.0331, 0.0913],\n",
              "         [0.0869, 0.4842, 0.4289]]),\n",
              " tensor([[ 0.6663, -1.4628, -0.1123],\n",
              "         [ 1.7150, -1.5601, -0.5463],\n",
              "         [-0.4036,  1.3142,  1.1931]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential(\n",
        "    Linear(3, 10),\n",
        "    ReLU(),\n",
        "    Linear(10, 20),\n",
        "    ReLU(),\n",
        "    Linear(20, 1),\n",
        ")\n",
        "print(model(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn0FzXMwehrR",
        "outputId": "7e1c8c84-0fe5-4eb2-9333-7d0fbbf02e38"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2689],\n",
            "        [0.2980],\n",
            "        [0.1061]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ф.Потерь\n"
      ],
      "metadata": {
        "id": "IGNDdwVVeQaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = MSELoss()\n",
        "y = torch.tensor([[0.5],[1.5],[0.7]])\n",
        "y_hat = torch.tensor([[0.6],[0.2],[0.5]])\n",
        "print(loss_fn(y_hat, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1JIS3itgH3o",
        "outputId": "4013e049-696b-4cb4-e6e5-b14db8d83b7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = BCELoss()\n",
        "y = torch.tensor([[1],[1.],[0.]])\n",
        "y_hat = torch.tensor([[0.9],[0.3],[0.2]])\n",
        "print(loss_fn(y_hat, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0ioo9B4gl73",
        "outputId": "418d7c7c-dc0b-4312-bb7b-fda77ddbf0a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = CrossEntropyLoss()\n",
        "y = torch.tensor([1, 2, 0])\n",
        "y_hat = torch.randn(3,3).abs()\n",
        "print(loss_fn(y_hat, y))\n",
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2HyqU-Qg2jk",
        "outputId": "d7f7add4-6f66-459f-9a57-5df4c0d0e0e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8355)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5839, 0.5406, 0.5191],\n",
              "        [0.5009, 1.2433, 2.5754],\n",
              "        [0.7973, 0.2457, 1.0920]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Метрики"
      ],
      "metadata": {
        "id": "nJMCsIqih4-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([[0.5],[1.5],[0.7]])\n",
        "y_hat = torch.tensor([[0.6],[0.2],[0.5]])\n",
        "\n",
        "score = (y-y_hat).abs() / y * 100\n",
        "score.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvQb0Ql7iDHD",
        "outputId": "dc3a81c6-2dbf-4c09-d2ca-ff983cbae65d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(45.0794)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([[1.],[1.],[0.]])\n",
        "y_hat = torch.tensor([[0.9],[0.3],[0.2]])\n",
        "\n",
        "score = (y_hat.round() == y).sum() / len(y) * 100\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95GxbVr3iZZd",
        "outputId": "a2fc844e-61e7-4e64-e7b5-c0f8b8bf995a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(66.6667)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([1, 2, 0])\n",
        "y_hat = torch.tensor([\n",
        "    [-1.2, 2.2, 0.2],\n",
        "    [2.3, 0.2, 0.7],\n",
        "    [0.9, -2.2, -1.2]\n",
        "])\n",
        "\n",
        "score = (y_hat.argmax(1) == y).sum() / len(y) * 100\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M3LNNySiwkG",
        "outputId": "7f665c08-53ac-49dd-a135-6242b4ef2ee7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(66.6667)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение нейросети"
      ],
      "metadata": {
        "id": "8GDeDOIYjgsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.load('data.pt')\n",
        "Y = torch.load('target.pt')\n",
        "X[:3], Y[:3], len(X)"
      ],
      "metadata": {
        "id": "SmTe9F1GuR7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bd8fee-b14c-4c28-aa61-94944c7b137f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[7.4000, 2.8000, 6.1000, 1.9000],\n",
              "         [6.7000, 3.1000, 5.6000, 2.4000],\n",
              "         [6.0000, 3.4000, 4.5000, 1.6000]]),\n",
              " tensor([2, 2, 1]),\n",
              " 150)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_value = X[:100], X[100:]\n",
        "Y_train, Y_value = Y[:100], Y[100:]\n",
        "\n",
        "model = Sequential(Linear(4, 16),\n",
        "                   ReLU(),\n",
        "                   Linear(16, 3))\n",
        "\n",
        "loss_fn = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "D7wSm5DBwLdL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.02)"
      ],
      "metadata": {
        "id": "W7USx_X5xnWt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(1000):\n",
        "  loss = loss_fn(model(X_train), Y_train)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  print(f'{loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HTs10zkX0rg2",
        "outputId": "c4d9cd67-aadb-4ce1-ca92-7c75a025a642"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.388\n",
            "1.092\n",
            "0.994\n",
            "0.961\n",
            "0.948\n",
            "0.940\n",
            "0.933\n",
            "0.927\n",
            "0.922\n",
            "0.916\n",
            "0.911\n",
            "0.906\n",
            "0.900\n",
            "0.895\n",
            "0.890\n",
            "0.885\n",
            "0.880\n",
            "0.875\n",
            "0.869\n",
            "0.864\n",
            "0.859\n",
            "0.854\n",
            "0.849\n",
            "0.844\n",
            "0.839\n",
            "0.834\n",
            "0.829\n",
            "0.824\n",
            "0.819\n",
            "0.814\n",
            "0.809\n",
            "0.804\n",
            "0.799\n",
            "0.794\n",
            "0.789\n",
            "0.784\n",
            "0.779\n",
            "0.774\n",
            "0.769\n",
            "0.765\n",
            "0.760\n",
            "0.755\n",
            "0.750\n",
            "0.745\n",
            "0.741\n",
            "0.736\n",
            "0.731\n",
            "0.727\n",
            "0.722\n",
            "0.718\n",
            "0.713\n",
            "0.709\n",
            "0.704\n",
            "0.700\n",
            "0.696\n",
            "0.691\n",
            "0.687\n",
            "0.683\n",
            "0.679\n",
            "0.675\n",
            "0.671\n",
            "0.666\n",
            "0.662\n",
            "0.659\n",
            "0.655\n",
            "0.651\n",
            "0.647\n",
            "0.643\n",
            "0.639\n",
            "0.636\n",
            "0.632\n",
            "0.628\n",
            "0.625\n",
            "0.621\n",
            "0.618\n",
            "0.615\n",
            "0.611\n",
            "0.608\n",
            "0.605\n",
            "0.601\n",
            "0.598\n",
            "0.595\n",
            "0.592\n",
            "0.589\n",
            "0.586\n",
            "0.583\n",
            "0.580\n",
            "0.577\n",
            "0.574\n",
            "0.571\n",
            "0.568\n",
            "0.566\n",
            "0.563\n",
            "0.560\n",
            "0.558\n",
            "0.555\n",
            "0.552\n",
            "0.550\n",
            "0.547\n",
            "0.545\n",
            "0.542\n",
            "0.540\n",
            "0.538\n",
            "0.535\n",
            "0.533\n",
            "0.531\n",
            "0.528\n",
            "0.526\n",
            "0.524\n",
            "0.522\n",
            "0.520\n",
            "0.517\n",
            "0.515\n",
            "0.513\n",
            "0.511\n",
            "0.509\n",
            "0.507\n",
            "0.505\n",
            "0.503\n",
            "0.501\n",
            "0.499\n",
            "0.497\n",
            "0.496\n",
            "0.494\n",
            "0.492\n",
            "0.490\n",
            "0.488\n",
            "0.487\n",
            "0.485\n",
            "0.483\n",
            "0.481\n",
            "0.480\n",
            "0.478\n",
            "0.476\n",
            "0.475\n",
            "0.473\n",
            "0.471\n",
            "0.470\n",
            "0.468\n",
            "0.467\n",
            "0.465\n",
            "0.463\n",
            "0.462\n",
            "0.460\n",
            "0.459\n",
            "0.457\n",
            "0.456\n",
            "0.454\n",
            "0.453\n",
            "0.452\n",
            "0.450\n",
            "0.449\n",
            "0.447\n",
            "0.446\n",
            "0.445\n",
            "0.443\n",
            "0.442\n",
            "0.440\n",
            "0.439\n",
            "0.438\n",
            "0.436\n",
            "0.435\n",
            "0.434\n",
            "0.433\n",
            "0.431\n",
            "0.430\n",
            "0.429\n",
            "0.427\n",
            "0.426\n",
            "0.425\n",
            "0.424\n",
            "0.422\n",
            "0.421\n",
            "0.420\n",
            "0.419\n",
            "0.418\n",
            "0.416\n",
            "0.415\n",
            "0.414\n",
            "0.413\n",
            "0.412\n",
            "0.410\n",
            "0.409\n",
            "0.408\n",
            "0.407\n",
            "0.406\n",
            "0.405\n",
            "0.404\n",
            "0.403\n",
            "0.401\n",
            "0.400\n",
            "0.399\n",
            "0.398\n",
            "0.397\n",
            "0.396\n",
            "0.395\n",
            "0.394\n",
            "0.393\n",
            "0.392\n",
            "0.391\n",
            "0.390\n",
            "0.388\n",
            "0.387\n",
            "0.386\n",
            "0.385\n",
            "0.384\n",
            "0.383\n",
            "0.382\n",
            "0.381\n",
            "0.380\n",
            "0.379\n",
            "0.378\n",
            "0.377\n",
            "0.376\n",
            "0.375\n",
            "0.374\n",
            "0.373\n",
            "0.372\n",
            "0.371\n",
            "0.370\n",
            "0.369\n",
            "0.368\n",
            "0.367\n",
            "0.366\n",
            "0.366\n",
            "0.365\n",
            "0.364\n",
            "0.363\n",
            "0.362\n",
            "0.361\n",
            "0.360\n",
            "0.359\n",
            "0.358\n",
            "0.357\n",
            "0.356\n",
            "0.355\n",
            "0.354\n",
            "0.353\n",
            "0.353\n",
            "0.352\n",
            "0.351\n",
            "0.350\n",
            "0.349\n",
            "0.348\n",
            "0.347\n",
            "0.346\n",
            "0.345\n",
            "0.345\n",
            "0.344\n",
            "0.343\n",
            "0.342\n",
            "0.341\n",
            "0.340\n",
            "0.339\n",
            "0.338\n",
            "0.338\n",
            "0.337\n",
            "0.336\n",
            "0.335\n",
            "0.334\n",
            "0.333\n",
            "0.333\n",
            "0.332\n",
            "0.331\n",
            "0.330\n",
            "0.329\n",
            "0.328\n",
            "0.328\n",
            "0.327\n",
            "0.326\n",
            "0.325\n",
            "0.324\n",
            "0.324\n",
            "0.323\n",
            "0.322\n",
            "0.321\n",
            "0.320\n",
            "0.320\n",
            "0.319\n",
            "0.318\n",
            "0.317\n",
            "0.316\n",
            "0.316\n",
            "0.315\n",
            "0.314\n",
            "0.313\n",
            "0.312\n",
            "0.312\n",
            "0.311\n",
            "0.310\n",
            "0.309\n",
            "0.309\n",
            "0.308\n",
            "0.307\n",
            "0.306\n",
            "0.306\n",
            "0.305\n",
            "0.304\n",
            "0.303\n",
            "0.303\n",
            "0.302\n",
            "0.301\n",
            "0.300\n",
            "0.300\n",
            "0.299\n",
            "0.298\n",
            "0.298\n",
            "0.297\n",
            "0.296\n",
            "0.295\n",
            "0.295\n",
            "0.294\n",
            "0.293\n",
            "0.293\n",
            "0.292\n",
            "0.291\n",
            "0.290\n",
            "0.290\n",
            "0.289\n",
            "0.288\n",
            "0.288\n",
            "0.287\n",
            "0.286\n",
            "0.286\n",
            "0.285\n",
            "0.284\n",
            "0.284\n",
            "0.283\n",
            "0.282\n",
            "0.282\n",
            "0.281\n",
            "0.280\n",
            "0.280\n",
            "0.279\n",
            "0.278\n",
            "0.278\n",
            "0.277\n",
            "0.276\n",
            "0.276\n",
            "0.275\n",
            "0.274\n",
            "0.274\n",
            "0.273\n",
            "0.272\n",
            "0.272\n",
            "0.271\n",
            "0.271\n",
            "0.270\n",
            "0.269\n",
            "0.269\n",
            "0.268\n",
            "0.267\n",
            "0.267\n",
            "0.266\n",
            "0.266\n",
            "0.265\n",
            "0.264\n",
            "0.264\n",
            "0.263\n",
            "0.263\n",
            "0.262\n",
            "0.261\n",
            "0.261\n",
            "0.260\n",
            "0.260\n",
            "0.259\n",
            "0.258\n",
            "0.258\n",
            "0.257\n",
            "0.257\n",
            "0.256\n",
            "0.255\n",
            "0.255\n",
            "0.254\n",
            "0.254\n",
            "0.253\n",
            "0.253\n",
            "0.252\n",
            "0.251\n",
            "0.251\n",
            "0.250\n",
            "0.250\n",
            "0.249\n",
            "0.249\n",
            "0.248\n",
            "0.247\n",
            "0.247\n",
            "0.246\n",
            "0.246\n",
            "0.245\n",
            "0.245\n",
            "0.244\n",
            "0.244\n",
            "0.243\n",
            "0.242\n",
            "0.242\n",
            "0.241\n",
            "0.241\n",
            "0.240\n",
            "0.240\n",
            "0.239\n",
            "0.239\n",
            "0.238\n",
            "0.238\n",
            "0.237\n",
            "0.237\n",
            "0.236\n",
            "0.236\n",
            "0.235\n",
            "0.235\n",
            "0.234\n",
            "0.234\n",
            "0.233\n",
            "0.233\n",
            "0.232\n",
            "0.231\n",
            "0.231\n",
            "0.230\n",
            "0.230\n",
            "0.229\n",
            "0.229\n",
            "0.228\n",
            "0.228\n",
            "0.228\n",
            "0.227\n",
            "0.227\n",
            "0.226\n",
            "0.226\n",
            "0.225\n",
            "0.225\n",
            "0.224\n",
            "0.224\n",
            "0.223\n",
            "0.223\n",
            "0.222\n",
            "0.222\n",
            "0.221\n",
            "0.221\n",
            "0.220\n",
            "0.220\n",
            "0.219\n",
            "0.219\n",
            "0.219\n",
            "0.218\n",
            "0.218\n",
            "0.217\n",
            "0.217\n",
            "0.216\n",
            "0.216\n",
            "0.215\n",
            "0.215\n",
            "0.215\n",
            "0.214\n",
            "0.214\n",
            "0.213\n",
            "0.213\n",
            "0.212\n",
            "0.212\n",
            "0.212\n",
            "0.211\n",
            "0.211\n",
            "0.210\n",
            "0.210\n",
            "0.209\n",
            "0.209\n",
            "0.209\n",
            "0.208\n",
            "0.208\n",
            "0.207\n",
            "0.207\n",
            "0.207\n",
            "0.206\n",
            "0.206\n",
            "0.205\n",
            "0.205\n",
            "0.205\n",
            "0.204\n",
            "0.204\n",
            "0.203\n",
            "0.203\n",
            "0.203\n",
            "0.202\n",
            "0.202\n",
            "0.201\n",
            "0.201\n",
            "0.201\n",
            "0.200\n",
            "0.200\n",
            "0.200\n",
            "0.199\n",
            "0.199\n",
            "0.198\n",
            "0.198\n",
            "0.198\n",
            "0.197\n",
            "0.197\n",
            "0.197\n",
            "0.196\n",
            "0.196\n",
            "0.195\n",
            "0.195\n",
            "0.195\n",
            "0.194\n",
            "0.194\n",
            "0.194\n",
            "0.193\n",
            "0.193\n",
            "0.193\n",
            "0.192\n",
            "0.192\n",
            "0.192\n",
            "0.191\n",
            "0.191\n",
            "0.191\n",
            "0.190\n",
            "0.190\n",
            "0.189\n",
            "0.189\n",
            "0.189\n",
            "0.188\n",
            "0.188\n",
            "0.188\n",
            "0.187\n",
            "0.187\n",
            "0.187\n",
            "0.186\n",
            "0.186\n",
            "0.186\n",
            "0.186\n",
            "0.185\n",
            "0.185\n",
            "0.185\n",
            "0.184\n",
            "0.184\n",
            "0.184\n",
            "0.183\n",
            "0.183\n",
            "0.183\n",
            "0.182\n",
            "0.182\n",
            "0.182\n",
            "0.181\n",
            "0.181\n",
            "0.181\n",
            "0.180\n",
            "0.180\n",
            "0.180\n",
            "0.180\n",
            "0.179\n",
            "0.179\n",
            "0.179\n",
            "0.178\n",
            "0.178\n",
            "0.178\n",
            "0.177\n",
            "0.177\n",
            "0.177\n",
            "0.177\n",
            "0.176\n",
            "0.176\n",
            "0.176\n",
            "0.175\n",
            "0.175\n",
            "0.175\n",
            "0.175\n",
            "0.174\n",
            "0.174\n",
            "0.174\n",
            "0.173\n",
            "0.173\n",
            "0.173\n",
            "0.173\n",
            "0.172\n",
            "0.172\n",
            "0.172\n",
            "0.172\n",
            "0.171\n",
            "0.171\n",
            "0.171\n",
            "0.170\n",
            "0.170\n",
            "0.170\n",
            "0.170\n",
            "0.169\n",
            "0.169\n",
            "0.169\n",
            "0.169\n",
            "0.168\n",
            "0.168\n",
            "0.168\n",
            "0.168\n",
            "0.167\n",
            "0.167\n",
            "0.167\n",
            "0.167\n",
            "0.166\n",
            "0.166\n",
            "0.166\n",
            "0.166\n",
            "0.165\n",
            "0.165\n",
            "0.165\n",
            "0.165\n",
            "0.164\n",
            "0.164\n",
            "0.164\n",
            "0.164\n",
            "0.163\n",
            "0.163\n",
            "0.163\n",
            "0.163\n",
            "0.162\n",
            "0.162\n",
            "0.162\n",
            "0.162\n",
            "0.161\n",
            "0.161\n",
            "0.161\n",
            "0.161\n",
            "0.160\n",
            "0.160\n",
            "0.160\n",
            "0.160\n",
            "0.160\n",
            "0.159\n",
            "0.159\n",
            "0.159\n",
            "0.159\n",
            "0.158\n",
            "0.158\n",
            "0.158\n",
            "0.158\n",
            "0.158\n",
            "0.157\n",
            "0.157\n",
            "0.157\n",
            "0.157\n",
            "0.156\n",
            "0.156\n",
            "0.156\n",
            "0.156\n",
            "0.156\n",
            "0.155\n",
            "0.155\n",
            "0.155\n",
            "0.155\n",
            "0.154\n",
            "0.154\n",
            "0.154\n",
            "0.154\n",
            "0.154\n",
            "0.153\n",
            "0.153\n",
            "0.153\n",
            "0.153\n",
            "0.153\n",
            "0.152\n",
            "0.152\n",
            "0.152\n",
            "0.152\n",
            "0.152\n",
            "0.151\n",
            "0.151\n",
            "0.151\n",
            "0.151\n",
            "0.151\n",
            "0.150\n",
            "0.150\n",
            "0.150\n",
            "0.150\n",
            "0.150\n",
            "0.149\n",
            "0.149\n",
            "0.149\n",
            "0.149\n",
            "0.149\n",
            "0.148\n",
            "0.148\n",
            "0.148\n",
            "0.148\n",
            "0.148\n",
            "0.147\n",
            "0.147\n",
            "0.147\n",
            "0.147\n",
            "0.147\n",
            "0.147\n",
            "0.146\n",
            "0.146\n",
            "0.146\n",
            "0.146\n",
            "0.146\n",
            "0.145\n",
            "0.145\n",
            "0.145\n",
            "0.145\n",
            "0.145\n",
            "0.145\n",
            "0.144\n",
            "0.144\n",
            "0.144\n",
            "0.144\n",
            "0.144\n",
            "0.143\n",
            "0.143\n",
            "0.143\n",
            "0.143\n",
            "0.143\n",
            "0.143\n",
            "0.142\n",
            "0.142\n",
            "0.142\n",
            "0.142\n",
            "0.142\n",
            "0.142\n",
            "0.141\n",
            "0.141\n",
            "0.141\n",
            "0.141\n",
            "0.141\n",
            "0.141\n",
            "0.140\n",
            "0.140\n",
            "0.140\n",
            "0.140\n",
            "0.140\n",
            "0.140\n",
            "0.139\n",
            "0.139\n",
            "0.139\n",
            "0.139\n",
            "0.139\n",
            "0.139\n",
            "0.138\n",
            "0.138\n",
            "0.138\n",
            "0.138\n",
            "0.138\n",
            "0.138\n",
            "0.138\n",
            "0.137\n",
            "0.137\n",
            "0.137\n",
            "0.137\n",
            "0.137\n",
            "0.137\n",
            "0.136\n",
            "0.136\n",
            "0.136\n",
            "0.136\n",
            "0.136\n",
            "0.136\n",
            "0.136\n",
            "0.135\n",
            "0.135\n",
            "0.135\n",
            "0.135\n",
            "0.135\n",
            "0.135\n",
            "0.134\n",
            "0.134\n",
            "0.134\n",
            "0.134\n",
            "0.134\n",
            "0.134\n",
            "0.134\n",
            "0.133\n",
            "0.133\n",
            "0.133\n",
            "0.133\n",
            "0.133\n",
            "0.133\n",
            "0.133\n",
            "0.132\n",
            "0.132\n",
            "0.132\n",
            "0.132\n",
            "0.132\n",
            "0.132\n",
            "0.132\n",
            "0.131\n",
            "0.131\n",
            "0.131\n",
            "0.131\n",
            "0.131\n",
            "0.131\n",
            "0.131\n",
            "0.131\n",
            "0.130\n",
            "0.130\n",
            "0.130\n",
            "0.130\n",
            "0.130\n",
            "0.130\n",
            "0.130\n",
            "0.129\n",
            "0.129\n",
            "0.129\n",
            "0.129\n",
            "0.129\n",
            "0.129\n",
            "0.129\n",
            "0.129\n",
            "0.128\n",
            "0.128\n",
            "0.128\n",
            "0.128\n",
            "0.128\n",
            "0.128\n",
            "0.128\n",
            "0.128\n",
            "0.127\n",
            "0.127\n",
            "0.127\n",
            "0.127\n",
            "0.127\n",
            "0.127\n",
            "0.127\n",
            "0.127\n",
            "0.126\n",
            "0.126\n",
            "0.126\n",
            "0.126\n",
            "0.126\n",
            "0.126\n",
            "0.126\n",
            "0.126\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.124\n",
            "0.124\n",
            "0.124\n",
            "0.124\n",
            "0.124\n",
            "0.124\n",
            "0.124\n",
            "0.124\n",
            "0.124\n",
            "0.123\n",
            "0.123\n",
            "0.123\n",
            "0.123\n",
            "0.123\n",
            "0.123\n",
            "0.123\n",
            "0.123\n",
            "0.123\n",
            "0.122\n",
            "0.122\n",
            "0.122\n",
            "0.122\n",
            "0.122\n",
            "0.122\n",
            "0.122\n",
            "0.122\n",
            "0.122\n",
            "0.121\n",
            "0.121\n",
            "0.121\n",
            "0.121\n",
            "0.121\n",
            "0.121\n",
            "0.121\n",
            "0.121\n",
            "0.121\n",
            "0.120\n",
            "0.120\n",
            "0.120\n",
            "0.120\n",
            "0.120\n",
            "0.120\n",
            "0.120\n",
            "0.120\n",
            "0.120\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.119\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.118\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.117\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.116\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.115\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.114\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.113\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.112\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.111\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.110\n",
            "0.109\n",
            "0.109\n",
            "0.109\n",
            "0.109\n",
            "0.109\n",
            "0.109\n",
            "0.109\n",
            "0.109\n",
            "0.109\n",
            "0.109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = (model(X_value).argmax(1) == Y_value).sum() / len(Y_value) * 100\n",
        "f'точность модели: {score:.0f}%'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PImjONZF1F0M",
        "outputId": "273638d9-8088-4f9e-ccba-58c5c464c3b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'точность модели: 96%'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}